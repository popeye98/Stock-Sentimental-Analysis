{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making glove vectors for embedding layer\n",
    "#  https://www.dlology.com/blog/simple-stock-sentiment-analysis-with-news-data-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle \n",
    "import json\n",
    "import numpy as np\n",
    "FilePAth=r''\n",
    "FilePath=''\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU,LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "with open(FilePAth+'\\glove.6B.50d.txt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        \n",
    "      \n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:],dtype='float32')\n",
    "        \n",
    "#         print(word)\n",
    "#         print(coeffs)\n",
    "        embeddings[word] = coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(FilePAth+'embeddings_dict.txt','w') as f:\n",
    "#   f.write(str(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings=None\n",
    "# with open(FilePAth+'embeddings_dict.txt','r') as f:\n",
    "#   train_descriptions=f.read()\n",
    "# json_sting=embeddings.replace(\"'\",\"\\\"\")\n",
    "# embeddings=json.loads(json_sting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "with open(FilePath+'testheadlines.pkl', 'rb') as f:\n",
    "    X_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "with open(FilePath+'trainheadlines.pkl', 'rb') as f:\n",
    "    X_train=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "with open(FilePath+'y_train.pkl', 'rb') as f:\n",
    "    y_train=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=[]\n",
    "with open(FilePath+'y_test.pkl', 'rb') as f:\n",
    "    y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now deciding the number of max len to be choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25fa6d3b188>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lengths = list(map(lambda x: len(x.split()), X_train))\n",
    "df = pd.DataFrame({'counts': X_train_lengths})\n",
    "df.counts.plot.hist(bins = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEnce most of the string have length of 500 and maximum of 700 so padding string to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(FilePAth+'\\glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map['cat'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "  \n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
    "    X_indices = np.zeros((m, max_len), dtype=int)\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                # Increment j to j + 1\n",
    "                j += 1\n",
    "                if j >= max_len:\n",
    "                    break\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['germany to meet with france']\n",
      "X1_indices = [[160180 360915 239785 388711 152927      0      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array(['germany to meet with france'])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 9)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len=len(word_to_index)+1\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]   #50\n",
    "    \n",
    "    # now create embeddings for all word in X\n",
    "    \n",
    "    emb_matrix =np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    for word,index in word_to_index.items():\n",
    "        emb_matrix[index,:]=word_to_vec_map[word]\n",
    "        \n",
    "    # now create embedding layer which is alredy trained to emd_matrix\n",
    "    # so set embedding layer trainable =false\n",
    "    \n",
    "    embedding_layer =Embedding(vocab_len,emb_dim,trainable=False)\n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # set weights of embedding layer or embedding matrix\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45281 , -0.50108 , -0.53714 , -0.015697,  0.22191 ,  0.54602 ,\n",
       "       -0.67301 , -0.6891  ,  0.63493 , -0.19726 ,  0.33685 ,  0.7735  ,\n",
       "        0.90094 ,  0.38488 ,  0.38367 ,  0.2657  , -0.08057 ,  0.61089 ,\n",
       "       -1.2894  , -0.22313 , -0.61578 ,  0.21697 ,  0.35614 ,  0.44499 ,\n",
       "        0.60885 , -1.1633  , -1.1579  ,  0.36118 ,  0.10466 , -0.78325 ,\n",
       "        1.4352  ,  0.18629 , -0.26112 ,  0.83275 , -0.23123 ,  0.32481 ,\n",
       "        0.14485 , -0.44552 ,  0.33497 , -0.95946 , -0.097479,  0.48138 ,\n",
       "       -0.43352 ,  0.69455 ,  0.91043 , -0.28173 ,  0.41637 , -1.2609  ,\n",
       "        0.71278 ,  0.23782 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "embedding_layer.get_weights()[0][word_to_index['cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(pretrained_embedding_layer(word_to_vec_map, word_to_index))\n",
    "model.add(LSTM(128, dropout=0.3, return_sequences=True)) \n",
    "model.add(LSTM(128, dropout=0.3))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         91648     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,411\n",
      "Trainable params: 223,361\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxlen)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = y_train.reshape((-1,1))\n",
    "Y_test = y_test.reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 1591 samples, validate on 398 samples\n",
      "Epoch 1/10\n",
      "1591/1591 [==============================] - 104s 65ms/step - loss: 0.6956 - accuracy: 0.5261 - val_loss: 0.6941 - val_accuracy: 0.5050\n",
      "Epoch 2/10\n",
      "1591/1591 [==============================] - 92s 58ms/step - loss: 0.6910 - accuracy: 0.5399 - val_loss: 0.6971 - val_accuracy: 0.5050\n",
      "Epoch 3/10\n",
      "1591/1591 [==============================] - 100s 63ms/step - loss: 0.6880 - accuracy: 0.5475 - val_loss: 0.6954 - val_accuracy: 0.5075\n",
      "Epoch 4/10\n",
      "1591/1591 [==============================] - 99s 62ms/step - loss: 0.6859 - accuracy: 0.5531 - val_loss: 0.7021 - val_accuracy: 0.5126\n",
      "Epoch 5/10\n",
      "1591/1591 [==============================] - 99s 62ms/step - loss: 0.6876 - accuracy: 0.5512 - val_loss: 0.6949 - val_accuracy: 0.5151\n",
      "Epoch 6/10\n",
      "1591/1591 [==============================] - 100s 63ms/step - loss: 0.6834 - accuracy: 0.5651 - val_loss: 0.6957 - val_accuracy: 0.5101\n",
      "Epoch 7/10\n",
      "1591/1591 [==============================] - 118s 74ms/step - loss: 0.6817 - accuracy: 0.5676 - val_loss: 0.6988 - val_accuracy: 0.5151\n",
      "Epoch 8/10\n",
      "1591/1591 [==============================] - 110s 69ms/step - loss: 0.6765 - accuracy: 0.5732 - val_loss: 0.7036 - val_accuracy: 0.5101\n",
      "Epoch 9/10\n",
      "1591/1591 [==============================] - 121s 76ms/step - loss: 0.6726 - accuracy: 0.5745 - val_loss: 0.7009 - val_accuracy: 0.5176\n",
      "Epoch 10/10\n",
      "1591/1591 [==============================] - 102s 64ms/step - loss: 0.6740 - accuracy: 0.5688 - val_loss: 0.7155 - val_accuracy: 0.5176\n",
      "398/398 [==============================] - 4s 9ms/step\n",
      "Test score: 0.715467710291321\n",
      "Test accuracy: 0.5175879597663879\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "history = model.fit(X_train_indices, Y_train, batch_size=batch_size, epochs=10,\n",
    "          validation_data=(X_test_indices, Y_test))\n",
    "\n",
    "model.save(\"./model.h5\")\n",
    "score, acc = model.evaluate(X_test_indices, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
